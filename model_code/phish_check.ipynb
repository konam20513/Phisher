{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plpqR8Z1qWns"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "EZ9vFjpy1Oln",
    "outputId": "0781fb86-aaf8-46de-ef7e-7d98cd70b3e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9968' max='9968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9968/9968 2:07:42, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.107800</td>\n",
       "      <td>0.110857</td>\n",
       "      <td>0.978790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1011' max='1011' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1011/1011 06:55]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.11085662245750427, 'eval_f1': 0.9787896090856569, 'eval_runtime': 416.1524, 'eval_samples_per_second': 38.858, 'eval_steps_per_second': 2.429, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nbest_model = RobertaForSequenceClassification.from_pretrained(\"./best_model\")\\nbest_tokenizer = RobertaTokenizer.from_pretrained(\"./best_model\")\\nFor using the saved model in a Google Chrome extension, you would need to use a server-side solution or a cloud-based API to connect your extension to the trained model.\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import TrainerCallback\n",
    "import os\n",
    "from transformers import TrainingArguments, Trainer\n",
    "os.makedirs(\"./best_model\", exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SaveBestModelCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.best_f1_score = 0\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        metrics = trainer.evaluate()\n",
    "        f1_score = metrics[\"eval_f1\"]\n",
    "\n",
    "        if f1_score > self.best_f1_score:\n",
    "          self.best_f1_score = f1_score\n",
    "          model.save_pretrained(\"./best_model\")\n",
    "          tokenizer.save_pretrained(\"./best_model\")\n",
    "          print(f\"New best model saved with F1 score: {f1_score}\")\n",
    "\n",
    "# Load and preprocess the data\n",
    "train_data = pd.read_csv(\"train_links.csv\", encoding='utf-8', encoding_errors='ignore')\n",
    "test_data = pd.read_csv(\"test_links.csv\", encoding='utf-8', encoding_errors='ignore')\n",
    "\n",
    "test_data=test_data[:16171]\n",
    "\n",
    "train_data=train_data[['email', 'label']]\n",
    "test_data=test_data[['email', 'label']]\n",
    "\n",
    "\n",
    "#print(len(train_data))\n",
    "#print(train_data[train_data['label'].isnull()])\n",
    "\n",
    "\n",
    "train_data['label'] = train_data['label'].astype(int)\n",
    "test_data['label'] = test_data['label'].astype(int)\n",
    "\n",
    "\n",
    "train_email_list=train_data[\"email\"].tolist()\n",
    "for i in range(len(train_email_list)):\n",
    "  if type(train_email_list[i]) != type('a'):\n",
    "    temp=str(train_email_list[i])\n",
    "    train_email_list[i]=temp\n",
    "\n",
    "\n",
    "train_label_list=train_data[\"label\"].tolist()\n",
    "\n",
    "#print(len(train_email_list))\n",
    "#print(len(train_label_list))\n",
    "\n",
    "\n",
    "for i in range(len(train_label_list)):\n",
    "  if type(train_label_list[i]) != type(1):\n",
    "    temp=int(train_label_list[i])\n",
    "    train_label_list[i]=temp\n",
    "\n",
    "count=0\n",
    "#print(count)\n",
    "for i in (train_data[\"label\"].tolist()):\n",
    "  if type(i) != type(1):\n",
    "    count+=1\n",
    "\n",
    "#print(count)\n",
    "\n",
    "#print(len(train_data))\n",
    "#print(train_data[train_data['label'].isnull()])\n",
    "\n",
    "\n",
    "\n",
    "test_email_list=test_data[\"email\"].tolist()\n",
    "for i in range(len(test_email_list)):\n",
    "  if type(test_email_list[i]) != type('a'):\n",
    "    temp=str(test_email_list[i])\n",
    "    test_email_list[i]=temp\n",
    "\n",
    "\n",
    "test_label_list=test_data[\"label\"].tolist()\n",
    "\n",
    "#print(len(train_email_list))\n",
    "#print(len(train_label_list))\n",
    "\n",
    "\n",
    "for i in range(len(test_label_list)):\n",
    "  if type(test_label_list[i]) != type(1):\n",
    "    temp=int(test_label_list[i])\n",
    "    test_label_list[i]=temp\n",
    "\n",
    "count=0\n",
    "#print(count)\n",
    "for i in (test_data[\"label\"].tolist()):\n",
    "  if type(i) != type(1):\n",
    "    count+=1\n",
    "\n",
    "#print(count)\n",
    "\n",
    "train_data=train_data[['email', 'label']]\n",
    "test_data=test_data[['email', 'label']]\n",
    "\n",
    "train_data['label'] = train_data['label'].astype(int)\n",
    "test_data['label'] = test_data['label'].astype(int)\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def preprocess(df):\n",
    "    inputs = tokenizer(df[\"email\"].tolist(), return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    labels = torch.tensor(df[\"label\"].tolist())\n",
    "    return inputs, labels\n",
    "\n",
    "train_inputs, train_labels = preprocess(train_data)\n",
    "test_inputs, test_labels = preprocess(test_data)\n",
    "\n",
    "# Custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.inputs.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "# Prepare the RoBERTa model for training\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "\n",
    "# Define the Trainer and TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    metrics = classification_report(labels, preds, output_dict=True)[\"weighted avg\"]\n",
    "    return {\"f1\": metrics[\"f1-score\"]}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=CustomDataset(train_inputs, train_labels),\n",
    "    eval_dataset=CustomDataset(test_inputs, test_labels),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "#trainer.add_callback(SaveBestModelCallback())\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"Evaluation results:\", eval_results)\n",
    "\n",
    "model.save_pretrained('./best_model')\n",
    "model.save_pretrained('./best_model.h5')\n",
    "tokenizer.save_pretrained(\"./best_model\")\n",
    "\n",
    "\"\"\"\n",
    "best_model = RobertaForSequenceClassification.from_pretrained(\"./best_model\")\n",
    "best_tokenizer = RobertaTokenizer.from_pretrained(\"./best_model\")\n",
    "For using the saved model in a Google Chrome extension, you would need to use a server-side solution or a cloud-based API to connect your extension to the trained model.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "9nCVi8GcYEnx"
   },
   "outputs": [],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"./best_model\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"./best_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "9nCVi8GcYEnx"
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"www.tiem.utk.edu/~gross/bioed/bealsmodules/spider.html\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
